{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca700f-626a-4feb-86c6-ea80a0303f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "import pandas as pd\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epochs = 50\n",
    "        self.num_users = 100\n",
    "        self.frac = 0.1\n",
    "        self.lr = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.model = 'cnn'\n",
    "        self.kernel_num = 9\n",
    "        self.kernel_sizes = '3,4,5'\n",
    "        self.num_channels = 1\n",
    "        self.norm = 'batch_norm'\n",
    "        self.num_filters = 32\n",
    "        self.max_pool = 'True'\n",
    "        self.dataset = 'mnist'\n",
    "        self.num_classes = 10\n",
    "        self.gpu = None\n",
    "        self.optimizer = 'sgd'\n",
    "        self.iid = 1\n",
    "        self.unequal = 0\n",
    "        self.stopping_rounds = 10\n",
    "        self.verbose = 1\n",
    "        self.seed = 1\n",
    "        self.gpu_id = None\n",
    "        self.use_fractional = True  # Using the fractional update rule\n",
    "        self.alpha = 0.99  # Fractional order parameter\n",
    "        self.mu0 = 0.1  # Initial learning rate\n",
    "        self.delta = 0.1  # Delta parameter\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def exp_details(args):\n",
    "    pass\n",
    "\n",
    "def mnist_iid(dataset, num_users):\n",
    "    num_items = int(len(dataset) / num_users)\n",
    "    dict_users = {i: set() for i in range(num_users)}\n",
    "    all_idxs = np.arange(len(dataset))\n",
    "    for i, idx in enumerate(all_idxs):\n",
    "        dict_users[i % num_users].add(idx)\n",
    "    return dict_users\n",
    "\n",
    "def mnist_noniid(dataset, num_users):\n",
    "    num_shards, num_imgs = 200, 300\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards * num_imgs)\n",
    "    labels = dataset.targets.numpy()\n",
    "\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand * num_imgs:(rand + 1) * num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "def get_dataset(args, iid):\n",
    "    data_dir = './data/' + args.dataset + '/'\n",
    "    apply_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=apply_transform)\n",
    "    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=apply_transform)\n",
    "\n",
    "    if iid:\n",
    "        user_groups = mnist_iid(train_dataset, args.num_users)\n",
    "    else:\n",
    "        user_groups = mnist_noniid(train_dataset, args.num_users)\n",
    "\n",
    "    return train_dataset, test_dataset, user_groups\n",
    "\n",
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "class LocalUpdateFractional(object):\n",
    "    def __init__(self, args, dataset, idxs, logger):\n",
    "        self.args = args\n",
    "        self.logger = logger\n",
    "        self.trainloader, self.validloader, self.testloader = self.train_val_test(dataset, list(idxs))\n",
    "        self.device = 'cuda' if args.gpu else 'cpu'\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
    "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
    "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
    "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train), batch_size=self.args.local_bs, shuffle=True)\n",
    "        validloader = DataLoader(DatasetSplit(dataset, idxs_val), batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "        testloader = DataLoader(DatasetSplit(dataset, idxs_test), batch_size=int(len(idxs_test)/10), shuffle=False)\n",
    "        return trainloader, validloader, testloader\n",
    "\n",
    "    def update_weights(self, model, global_round):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        t = torch.tensor(global_round, dtype=torch.float32)  # Global round as tensor\n",
    "        alpha = torch.tensor(self.args.alpha, dtype=torch.float32)  # Fractional order parameter\n",
    "        mu0 = torch.tensor(self.args.mu0, dtype=torch.float32)  # Initial learning rate\n",
    "        delta = torch.tensor(self.args.delta, dtype=torch.float32)  # Delta parameter\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr, momentum=self.args.momentum, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "        for iter in range(self.args.local_ep):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                model.zero_grad()\n",
    "                log_probs = model(images)\n",
    "                loss = self.criterion(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Fractional Order SGD Update\n",
    "                if global_round == 0:\n",
    "                    w_next = {name: param - mu0 * param.grad for name, param in model.named_parameters()}\n",
    "                else:\n",
    "                    w_prev = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "                    w_next = {name: param - (mu0 / torch.sqrt(t + 1)) * param.grad / torch.lgamma(2 - alpha).exp() *\n",
    "                                      (torch.abs(param - w_prev[name]) + delta)**(1 - alpha)\n",
    "                              for name, param in model.named_parameters()}\n",
    "\n",
    "                for name, param in model.named_parameters():\n",
    "                    param.data = w_next[name]\n",
    "\n",
    "                if self.args.verbose and (batch_idx % 10 == 0):\n",
    "                    self.logger.add_scalar('loss', loss.item())\n",
    "                batch_loss.append(loss.item())\n",
    "            scheduler.step()\n",
    "            epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "    def inference(self, model):\n",
    "        model.eval()\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(self.testloader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = model(images)\n",
    "            batch_loss = self.criterion(outputs, labels)\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy, loss\n",
    "\n",
    "class LocalUpdateFedAvg(object):\n",
    "    def __init__(self, args, dataset, idxs, logger):\n",
    "        self.args = args\n",
    "        self.logger = logger\n",
    "        self.trainloader, self.validloader, self.testloader = self.train_val_test(dataset, list(idxs))\n",
    "        self.device = 'cuda' if args.gpu else 'cpu'\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
    "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
    "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
    "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train), batch_size=self.args.local_bs, shuffle=True)\n",
    "        validloader = DataLoader(DatasetSplit(dataset, idxs_val), batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "        testloader = DataLoader(DatasetSplit(dataset, idxs_test), batch_size=int(len(idxs_test)/10), shuffle=False)\n",
    "        return trainloader, validloader, testloader\n",
    "\n",
    "    def update_weights(self, model, global_round):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
    "\n",
    "        for iter in range(self.args.local_ep):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                model.zero_grad()\n",
    "                log_probs = model(images)\n",
    "                loss = self.criterion(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "    def inference(self, model):\n",
    "        model.eval()\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(self.testloader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = model(images)\n",
    "            batch_loss = self.criterion(outputs, labels)\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy, loss\n",
    "\n",
    "def test_inference(args, model, test_dataset):\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    device = 'cuda' if args.gpu else 'cpu'\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    testloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, loss\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1] * x.shape[-2] * x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class CNNFashion_Mnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNFashion_Mnist, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    path_project = os.path.abspath('.')\n",
    "    logger = SummaryWriter('./logs')\n",
    "\n",
    "    exp_details(args)\n",
    "\n",
    "    if args.gpu_id:\n",
    "        torch.cuda.set_device(args.gpu_id)\n",
    "    device = 'cuda' if args.gpu else 'cpu'\n",
    "\n",
    "    configs = [(10, 5)]\n",
    "\n",
    "    results_iid = {'fractional': {}, 'fedavg': {}}\n",
    "    results_noniid = {'fractional': {}, 'fedavg': {}}\n",
    "\n",
    "    for local_bs, local_ep in configs:\n",
    "        args.local_bs = local_bs\n",
    "        args.local_ep = local_ep\n",
    "\n",
    "        train_dataset, test_dataset, user_groups_iid = get_dataset(args, iid=True)\n",
    "        train_dataset, test_dataset, user_groups_noniid = get_dataset(args, iid=False)\n",
    "\n",
    "        for is_iid, user_groups, results, update_class in [\n",
    "            (True, user_groups_iid, results_iid, LocalUpdateFractional),\n",
    "            (False, user_groups_noniid, results_noniid, LocalUpdateFractional)\n",
    "        ]:\n",
    "            if args.model == 'cnn':\n",
    "                if args.dataset == 'mnist':\n",
    "                    global_model = CNNMnist(args=args)\n",
    "                elif args.dataset == 'fmnist':\n",
    "                    global_model = CNNFashion_Mnist(args=args)\n",
    "                elif args.dataset == 'cifar':\n",
    "                    global_model = CNNCifar(args=args)\n",
    "            elif args.model == 'mlp':\n",
    "                img_size = train_dataset[0][0].shape\n",
    "                len_in = 1\n",
    "                for x in img_size:\n",
    "                    len_in *= x\n",
    "                global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "            else:\n",
    "                exit('Error: unrecognized model')\n",
    "\n",
    "            global_model.to(device)\n",
    "            global_model.train()\n",
    "\n",
    "            global_weights = global_model.state_dict()\n",
    "\n",
    "            train_loss, train_accuracy, test_accuracies = [], [], []\n",
    "\n",
    "            for epoch in tqdm(range(args.epochs)):\n",
    "                local_weights, local_losses = [], []\n",
    "\n",
    "                global_model.train()\n",
    "                m = max(int(args.frac * args.num_users), 1)\n",
    "                idxs_users = np.arange(args.num_users)[:m]\n",
    "\n",
    "                for idx in idxs_users:\n",
    "                    local_model = update_class(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                    w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "                    local_weights.append(copy.deepcopy(w))\n",
    "                    local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "                global_weights = average_weights(local_weights)\n",
    "                global_model.load_state_dict(global_weights)\n",
    "\n",
    "                loss_avg = sum(local_losses) / len(local_losses)\n",
    "                train_loss.append(loss_avg)\n",
    "\n",
    "                list_acc, list_loss = [], []\n",
    "                global_model.eval()\n",
    "                for c in range(args.num_users):\n",
    "                    local_model = update_class(args=args, dataset=train_dataset, idxs=user_groups[c], logger=logger)\n",
    "                    acc, loss = local_model.inference(model=global_model)\n",
    "                    list_acc.append(acc)\n",
    "                    list_loss.append(loss)\n",
    "                train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "\n",
    "                # Calculate and store test accuracy at each round\n",
    "                test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "                test_accuracies.append(test_acc)\n",
    "\n",
    "            config_name = f'B={local_bs}_E={local_ep}'\n",
    "            results['fractional'][config_name] = {\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracies': test_accuracies\n",
    "            }\n",
    "\n",
    "        for is_iid, user_groups, results, update_class in [\n",
    "            (True, user_groups_iid, results_iid, LocalUpdateFedAvg),\n",
    "            (False, user_groups_noniid, results_noniid, LocalUpdateFedAvg)\n",
    "        ]:\n",
    "            if args.model == 'cnn':\n",
    "                if args.dataset == 'mnist':\n",
    "                    global_model = CNNMnist(args=args)\n",
    "                elif args.dataset == 'fmnist':\n",
    "                    global_model = CNNFashion_Mnist(args=args)\n",
    "                elif args.dataset == 'cifar':\n",
    "                    global_model = CNNCifar(args=args)\n",
    "            elif args.model == 'mlp':\n",
    "                img_size = train_dataset[0][0].shape\n",
    "                len_in = 1\n",
    "                for x in img_size:\n",
    "                    len_in *= x\n",
    "                global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "            else:\n",
    "                exit('Error: unrecognized model')\n",
    "\n",
    "            global_model.to(device)\n",
    "            global_model.train()\n",
    "\n",
    "            global_weights = global_model.state_dict()\n",
    "\n",
    "            train_loss, train_accuracy, test_accuracies = [], [], []\n",
    "\n",
    "            for epoch in tqdm(range(args.epochs)):\n",
    "                local_weights, local_losses = [], []\n",
    "\n",
    "                global_model.train()\n",
    "                m = max(int(args.frac * args.num_users), 1)\n",
    "                idxs_users = np.arange(args.num_users)[:m]\n",
    "\n",
    "                for idx in idxs_users:\n",
    "                    local_model = update_class(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                    w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "                    local_weights.append(copy.deepcopy(w))\n",
    "                    local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "                global_weights = average_weights(local_weights)\n",
    "                global_model.load_state_dict(global_weights)\n",
    "\n",
    "                loss_avg = sum(local_losses) / len(local_losses)\n",
    "                train_loss.append(loss_avg)\n",
    "\n",
    "                list_acc, list_loss = [], []\n",
    "                global_model.eval()\n",
    "                for c in range(args.num_users):\n",
    "                    local_model = update_class(args=args, dataset=train_dataset, idxs=user_groups[c], logger=logger)\n",
    "                    acc, loss = local_model.inference(model=global_model)\n",
    "                    list_acc.append(acc)\n",
    "                    list_loss.append(loss)\n",
    "                train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "\n",
    "                # Calculate and store test accuracy at each round\n",
    "                test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "                test_accuracies.append(test_acc)\n",
    "\n",
    "            config_name = f'B={local_bs}_E={local_ep}'\n",
    "            results['fedavg'][config_name] = {\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracies': test_accuracies\n",
    "            }\n",
    "\n",
    "    # Convert results to DataFrames\n",
    "    def convert_to_df(results, metric):\n",
    "        df = pd.DataFrame()\n",
    "        for method, data in results.items():\n",
    "            df[method] = data[f'B={configs[0][0]}_E={configs[0][1]}'][metric]\n",
    "        return df\n",
    "\n",
    "    # Test Accuracy for IID\n",
    "    df_test_acc_iid = convert_to_df(results_iid, 'test_accuracies')\n",
    "    print(\"\\nTable 1: Test Accuracy for IID setting\")\n",
    "    print(df_test_acc_iid)\n",
    "\n",
    "    # Test Accuracy for Non-IID\n",
    "    df_test_acc_noniid = convert_to_df(results_noniid, 'test_accuracies')\n",
    "    print(\"\\nTable 2: Test Accuracy for Non-IID setting\")\n",
    "    print(df_test_acc_noniid)\n",
    "\n",
    "    # Training Loss for IID\n",
    "    df_train_loss_iid = convert_to_df(results_iid, 'train_loss')\n",
    "    print(\"\\nTable 3: Training Loss for IID setting\")\n",
    "    print(df_train_loss_iid)\n",
    "\n",
    "    # Training Loss for Non-IID\n",
    "    df_train_loss_noniid = convert_to_df(results_noniid, 'train_loss')\n",
    "    print(\"\\nTable 4: Training Loss for Non-IID setting\")\n",
    "    print(df_train_loss_noniid)\n",
    "\n",
    "    # Save these tables to CSV files\n",
    "    df_test_acc_iid.to_csv('test_accuracy_iid.csv', index=False)\n",
    "    df_test_acc_noniid.to_csv('test_accuracy_noniid.csv', index=False)\n",
    "    df_train_loss_iid.to_csv('training_loss_iid.csv', index=False)\n",
    "    df_train_loss_noniid.to_csv('training_loss_noniid.csv', index=False)\n",
    "\n",
    "    # Define the directory where you want to save the figures\n",
    "    save_dir = './results'\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Plotting for IID - Test Accuracy\n",
    "    plt.figure()\n",
    "    for method in results_iid:\n",
    "        plt.plot(range(1, args.epochs + 1), results_iid[method][f'B={configs[0][0]}_E={configs[0][1]}']['test_accuracies'], label=method)\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Test Accuracy vs. Communication Rounds (IID)')\n",
    "    plt.savefig(os.path.join(save_dir, 'test_accuracy_vs_comm_rounds_iid.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for Non-IID - Test Accuracy\n",
    "    plt.figure()\n",
    "    for method in results_noniid:\n",
    "        plt.plot(range(1, args.epochs + 1), results_noniid[method][f'B={configs[0][0]}_E={configs[0][1]}']['test_accuracies'], label=method)\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Test Accuracy vs. Communication Rounds (Non-IID)')\n",
    "    plt.savefig(os.path.join(save_dir, 'test_accuracy_vs_comm_rounds_noniid.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for IID - Training Loss\n",
    "    plt.figure()\n",
    "    for method in results_iid:\n",
    "        plt.plot(range(1, args.epochs + 1), results_iid[method][f'B={configs[0][0]}_E={configs[0][1]}']['train_loss'], label=method)\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss vs. Communication Rounds (IID)')\n",
    "    plt.savefig(os.path.join(save_dir, 'train_loss_vs_comm_rounds_iid.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for Non-IID - Training Loss\n",
    "    plt.figure()\n",
    "    for method in results_noniid:\n",
    "        plt.plot(range(1, args.epochs + 1), results_noniid[method][f'B={configs[0][0]}_E={configs[0][1]}']['train_loss'], label=method)\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss vs. Communication Rounds (Non-IID)')\n",
    "    plt.savefig(os.path.join(save_dir, 'train_loss_vs_comm_rounds_noniid.png'))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
